```json
{
  "date": "2020.09.19 20:30",
  "tags": ["编译原理","笔记","词法分析"]
}
```

学习了词法分析之后，就来一波实操，我们仿造golang的源代码做一个简单的词法分析，由于是学习，我们尽量做得清晰一点，不涉及优化部分，分别设计三个包token、scanner、lexer，token用来描述数学表达式最基础的组成，scanner是一个扫描器，lexer产生Token 。

# Token

```go
type Token struct {
	Lit  string
	Type int
	Pos  int
}
//token type
const (
	ILLEGAL  int = iota
	NUMBER       // 12345  123.45
	OPERATOR  //+-*/()
)


func (tok Token) IsNumber() bool    { return tok.Type == NUMBER }
func (tok Token) IsOperator() bool { return tok.Type == OPERATOR }

const LowestPrec = 0

//操作符的优先级
func (tok Token) Precedence() int {
	if !tok.IsOperator() {
		return LowestPrec
	}
	switch tok.Lit {
	case "+", "-":
		return 1
	case "*", "/":
		return 2
	case "(", ")":
		return 3
	}
	return LowestPrec
}
func (tok Token) String() string {
	return fmt.Sprintf("&Token{Lit:%s,Type:%v,Pos:%v}", tok.Lit,tok.Type,tok.Pos)
}
```

实现 Token 的 String 方法有助于输出清晰的Token串。

# scanner

```go

type Scanner struct {
	src    string // source
	ch     string // current 当前字符
	offset int    // character offset
	err    error
	eof    bool //扫描结束
}

func New(expr string) *Scanner {
	return &Scanner{
		src:    expr,
		ch:     string(expr[0]),
		offset: 0,
		err:    nil,
		eof:    false,
	}
}

func (s *Scanner) String() string {
	return fmt.Sprintf("&Scanner{\n\tsrc:%s\n\tch:%s\n\toffset:%v\n\teof:%v\n}\n", s.src, s.ch, s.offset, s.eof)
}

func (s *Scanner) EOF() bool {
	return s.eof
}
func (s *Scanner) Err() error {
	return s.err
}
func (s *Scanner) Scan() *token.Token {
	s.skipWhitespace()
	switch ch := s.ch; {
	case isOperator(ch):
		tok := &token.Token{
			Lit:  ch,
			Type: token.OPERATOR,
			Pos:  s.offset,
		}
		s.next()
		return tok
	case isDecimal(ch):
		lit, t, p := s.scanNumber()
		return &token.Token{
			Lit:  lit,
			Type: t,
			Pos:  p,
		}
	default:
		s.err = errors.New(fmt.Sprintf("expr err pos:%v", s.offset))
		return &token.Token{
			Lit:  "",
			Type: token.ILLEGAL,
			Pos:  s.offset,
		}
	}
}

func (s *Scanner) skipWhitespace() {
	for s.ch == " " || s.ch == "\t" || s.ch == "\n" {
		s.next()
	}
}
func (s *Scanner) next() {
	length := len(s.src) - 1
	if s.offset >= length {
		s.offset = length
		s.eof = true
		return
	}
	s.offset++
	s.ch = string(s.src[s.offset])
}
func (s *Scanner) peek() string {
	if s.offset < len(s.src)-1 {
		return string(s.src[s.offset+1])
	}
	return ""
}
func (s *Scanner) scanNumber() (string, int, int) {
	lit := s.ch
	t := token.NUMBER
	p := s.offset
	for {
		s.next()
		if isDecimal(s.ch) {
			lit += string(s.ch)
			continue
		}
		if s.ch == "." {
			if !isDecimal(s.peek()) {
				s.err = errors.New(fmt.Sprintf("expr err pos:%v", s.offset))
				return lit, token.ILLEGAL, p
			}
			lit += string(s.ch)
			continue
		}
		return lit, t, p
	}

}

func isOperator(cb string) bool {
	return cb == "+" || cb == "-" || cb == "*" || cb == "/" || cb == "(" || cb == ")"
}
func isDecimal(cb string) bool {
	return cb == "0" || cb == "1" || cb == "2" || cb == "3" || cb == "4" || cb == "5" || cb == "6" || cb == "7" || cb == "8" || cb == "9"
}
```

# lexer

```go

func Tokenizer(expr string) ([]*token.Token, error) {

	scan := scanner.New(expr)
	var tokens []*token.Token

	for {
		if scan.EOF() {
			break
		}
		tok := scan.Scan()
		if scan.Err() != nil {
			return tokens, scan.Err()
		}
		tokens = append(tokens, tok)
	}
	return tokens, nil
}
```
输出效果

```go
var src = "1+54*(9.5+10)"
tokens, err := lexer.Tokenizer(src)
if err != nil {
	fmt.Println(err)
	return
}
fmt.Println(tokens)
```

```
[
	&Token{Lit:1,Type:1,Pos:0} &Token{Lit:+,Type:2,Pos:1} &Token{Lit:54,Type:1,Pos:2} &Token{Lit:*,Type:2,Pos:4} &Token{Lit:(,Type:2,Pos:5} 
	&Token{Lit:9.5,Type:1,Pos:6} &Token{Lit:+,Type:2,Pos:9} &Token{Lit:10,Type:1,Pos:10} &Token{Lit:),Type:2,Pos:12}
]

```